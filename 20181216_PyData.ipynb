{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stop Reinventing Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following post was presented as a talk for the [IE@DS](https://www.facebook.com/groups/173376299978861/) community, and for the [PyData meetup](https://www.meetup.com/PyData-Tel-Aviv/events/256232456/).  \n",
    "All the resources for this post, including a runable notebook, can be found in the [github repo](https://github.com/DeanLa/dont_reinvent_pandas)  \n",
    "blog post version here:  \n",
    "\n",
    "\n",
    "<span style=\"font-size:2em\"> [DeanLa.com](http://deanla.com/)</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![slide1](slides/slide1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![slide2](slides/slide2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to show some nice ways modern Pandas makes your life easier. It is not about efficiency. I'm pretty sure using Pandas' built-in methods will be more efficient than reinventing pandas, but the main goal is to make the code easier to read, and more imoprtant - easier to write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![slide3](slides/slide3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:32:17.424200Z",
     "start_time": "2018-12-16T14:32:14.874930Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(['classic', 'ggplot', 'seaborn-poster', 'dean.style'])\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import my_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# First Hacks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data and a few housekeeping tasks. is the first place we can make our code more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:32:20.480961Z",
     "start_time": "2018-12-16T14:32:20.292591Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_io = pd.read_csv('./data.csv',index_col=0,parse_dates=['date_'])\n",
    "df_io.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:32:27.272880Z",
     "start_time": "2018-12-16T14:32:27.237021Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = df_io.copy().sort_values('date_').set_index('date_').drop(columns='val_updated')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful pipes!\n",
    "One line method chaining is hard to read and prone to human error, chaining each method in its own line makes it a lot more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:32:49.441409Z",
     "start_time": "2018-12-16T14:32:49.407660Z"
    }
   },
   "outputs": [],
   "source": [
    "df_io\\\n",
    ".copy()\\\n",
    ".sort_values('date_')\\\n",
    ".set_index('date_')\\\n",
    ".drop(columns='val_updated')\\\n",
    ".head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it has a problem. You can't comment out and even comment in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:32:51.193599Z",
     "start_time": "2018-12-16T14:32:51.160821Z"
    }
   },
   "outputs": [],
   "source": [
    "# This block will result in an error\n",
    "df_io\\\n",
    ".copy()\\ # This is an inline comment\n",
    "# This is a regular comment\n",
    ".sort_values('date_')\\\n",
    "# .set_index('date_')\\\n",
    ".drop(columns='val_updated')\\ \n",
    ".head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even an unnoticeable space character may break everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:32:55.553310Z",
     "start_time": "2018-12-16T14:32:55.526420Z"
    }
   },
   "outputs": [],
   "source": [
    "# This block will result in an error\n",
    "df_io\\\n",
    ".copy()\\\n",
    ".sort_values('date_')\\\n",
    ".set_index('date_')\\\n",
    ".drop(columns='val_updated')\\ \n",
    ".head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T20:59:13.384077Z",
     "start_time": "2018-10-23T20:59:13.380263Z"
    }
   },
   "source": [
    "## The Penny Drops\n",
    "I like those \"penny dropping\" moments, when you realize you knew everything that is presented, yet it is presented in a new way you never thought of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:33:04.893709Z",
     "start_time": "2018-12-16T14:33:04.868867Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can split these value inside ()\n",
    "users = (134856, 195373, 295817, 294003, 262166, 121066, 129678, 307120, 258759, 277922, 220794, 192312, 318486, 314631, 306448, 297059,206892,         169046, 181703, 146200, 199876, 247904, 250884, 282989, 234280, 202520,         138064, 133577, 301053, 242157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:33:42.150883Z",
     "start_time": "2018-12-16T14:33:42.119285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Penny Drop: We can also Split here\n",
    "df = (df_io\n",
    "        .copy() # This is an inline comment\n",
    "        # This is a regular comment\n",
    "        .sort_values('date_')\n",
    "        .set_index('date_')\n",
    "        .drop(columns='val_updated')   \n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map with dict\n",
    "A dict is a callable with $f(key) = value$, there for you can call `.map` with it. In this example I want to make int key codes into letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:33:43.106067Z",
     "start_time": "2018-12-16T14:33:43.076707Z"
    }
   },
   "outputs": [],
   "source": [
    "df.event_type.map(lambda x: x+3).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:33:43.472866Z",
     "start_time": "2018-12-16T14:33:43.440601Z"
    }
   },
   "outputs": [],
   "source": [
    "# A dict is also a callable\n",
    "df['event_type'] = df.event_type.map({\n",
    "    1:'A',\n",
    "    5:'B',\n",
    "    7:'C'\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# Time Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![slide3](slides/meme-headache.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "## Resample\n",
    "Task: How many events happen each hour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Old Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:34:12.779189Z",
     "start_time": "2018-12-16T14:34:12.738452Z"
    }
   },
   "outputs": [],
   "source": [
    "bad = df.copy()\n",
    "bad['day'] = bad.index.date\n",
    "bad['hour'] = bad.index.hour\n",
    "(bad\n",
    ".groupby(['day','hour'])\n",
    ".count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many lines of code\n",
    "* unneeded columns\n",
    "* Index is not a time anymore\n",
    "* **missing rows** (Did you notice?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Better Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:34:28.871116Z",
     "start_time": "2018-12-16T14:34:28.831196Z"
    }
   },
   "outputs": [],
   "source": [
    "df.resample('H').count() # H is for Hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's even better on non-round intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:34:38.218086Z",
     "start_time": "2018-12-16T14:34:38.189806Z"
    }
   },
   "outputs": [],
   "source": [
    "rs = df.resample('10T').count()\n",
    "# T is for Minute, and pandas understands 10 T, it will also under stand 11T if you wonder\n",
    "rs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Complete list of Pandas' time abbrevations](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Period.strftime.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice Easily\n",
    "Pandas will automatically make string into timestamps, and it will understand what you want it to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:34:51.722761Z",
     "start_time": "2018-12-16T14:34:51.688918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take only timestamp in the hour of 21:00.\n",
    "rs.loc['2018-10-09 21',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:34:54.464867Z",
     "start_time": "2018-12-16T14:34:54.437123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take all time stamps before 18:31\n",
    "rs.loc[:'2018-10-09 18:31',:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Windows: Rolling, Expanding, EWM\n",
    "If your Dataframe is indexed on a time index (Which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:35:29.496238Z",
     "start_time": "2018-12-16T14:35:27.571084Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use(['classic', 'ggplot', 'seaborn-poster', 'dean.style'])\n",
    "fig, ax = plt.subplots()\n",
    "rs.plot(ax=ax,linestyle='--')\n",
    "(rs\n",
    " .rolling(6)\n",
    " .mean()\n",
    " .rename(columns = {'event_type':'rolling mean'})\n",
    " .plot(ax=ax)\n",
    ")\n",
    "\n",
    "rs.expanding(6).mean().rename(columns = {'event_type':'expanding mean'}).plot(ax=ax)\n",
    "rs.ewm(6).mean().rename(columns = {'event_type':'ewm mean'}).plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Apply\n",
    "Intuitively, windows are like GroupBy, so you can apply anything you want after the grouping, e.g.: geometric mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:35:38.847937Z",
     "start_time": "2018-12-16T14:35:38.563673Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "rs.plot(ax=ax,linestyle='--')\n",
    "(rs\n",
    " .rolling(6)\n",
    " .apply(lambda x: np.power(np.product(x),1/len(x)),raw=True)\n",
    " .rename(columns = {'event_type':'Rolling Geometric Mean'})\n",
    " .plot(ax=ax)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine with GroupBy 🤯\n",
    "Pandas has no problem with groupby and resample together. It's as simple as `groupby[col1,col2]`. In our specific case, we want to cound events in an interval per event type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:37:29.129783Z",
     "start_time": "2018-12-16T14:37:29.016347Z"
    }
   },
   "outputs": [],
   "source": [
    "per_event = (df\n",
    "             .groupby('event_type')\n",
    "             .resample('15T')\n",
    "             .apply('count')\n",
    "             .rename(columns={'event_type':'amount'})\n",
    "            )\n",
    "per_event.groupby('event_type').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![meme1](slides/meme-scientist.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:37:31.618912Z",
     "start_time": "2018-12-16T14:37:31.587704Z"
    }
   },
   "outputs": [],
   "source": [
    "per_event.sort_values(by=['amount']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:37:53.390643Z",
     "start_time": "2018-12-16T14:37:53.359733Z"
    }
   },
   "outputs": [],
   "source": [
    "per_event.sort_index().head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:38:03.384040Z",
     "start_time": "2018-12-16T14:38:03.350323Z"
    }
   },
   "outputs": [],
   "source": [
    "per_event.sort_index(level=1).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Both <span style=\"color:red\">(New in 0.23)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T15:12:23.481796Z",
     "start_time": "2018-12-16T15:12:23.364923Z"
    }
   },
   "outputs": [],
   "source": [
    "per_event.sort_values(['amount','event_type'], ascending=(False, True)).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack, Unstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstack \n",
    "In this case, working with a wide format indexed on intervals, with event types as columns, will make a lot more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Old way\n",
    "Pivot table in modern pandas is more robust than it used to be. Still, it requires you to specify everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:39:04.850010Z",
     "start_time": "2018-12-16T14:39:04.501944Z"
    }
   },
   "outputs": [],
   "source": [
    "pt = pd.pivot_table(per_event,values = 'amount',columns='event_type',index='date_')\n",
    "pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T09:54:16.056427Z",
     "start_time": "2018-10-24T09:54:16.054110Z"
    }
   },
   "source": [
    "### A better way\n",
    "When you have just one column of values, unstack does the same easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:39:08.238577Z",
     "start_time": "2018-12-16T14:39:08.204960Z"
    }
   },
   "outputs": [],
   "source": [
    "pt = per_event.unstack('event_type')\n",
    "pt.columns = pt.columns.droplevel() # Unstack creates a multiindex on columns\n",
    "pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstack\n",
    "And some extra tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:39:22.687950Z",
     "start_time": "2018-12-16T14:39:22.650452Z"
    }
   },
   "outputs": [],
   "source": [
    "pt.stack().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks kind of what we had expected but:\n",
    "* It's a series, not a DataFrame\n",
    "* The levels of the index are reversed\n",
    "* The main sort is on the date, yet it used to be on the event type\n",
    "\n",
    "\n",
    "### Some More Hacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:40:28.705948Z",
     "start_time": "2018-12-16T14:40:28.674170Z"
    }
   },
   "outputs": [],
   "source": [
    "stack_back = (pt\n",
    "              .stack()\n",
    "              .to_frame('amount') # Turn Series to DF without calling the DF constructor\n",
    "              .swaplevel() # Swaps the levels of the index\n",
    "              .sort_index()\n",
    "             )\n",
    "stack_back.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:40:29.574427Z",
     "start_time": "2018-12-16T14:40:29.543873Z"
    }
   },
   "outputs": [],
   "source": [
    "stack_back.equals(per_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T21:23:37.783574Z",
     "start_time": "2018-10-23T21:23:37.780922Z"
    }
   },
   "source": [
    "# Clip\n",
    "Let's say, we know from domain knowledge the that an event takes place a minimum of 5 and maximum of 12 at each timestamp. We would like to fix that. In a real world example, we many time want to turn negative numbers to zeroes or some truly big numbers to sum known max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Old Way\n",
    "Iterate over columns and change values that meet condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:41:03.818117Z",
     "start_time": "2018-12-16T14:41:03.382509Z"
    }
   },
   "outputs": [],
   "source": [
    "cl = pt.copy()\n",
    "lb = 5\n",
    "ub = 12\n",
    "# Needed A loop of 3 lines\n",
    "for col in ['A','B','C']:\n",
    "    cl['clipped_{}'.format(col)] = cl[col]\n",
    "    cl.loc[cl[col] < lb,'clipped_{}'.format(col)] = lb\n",
    "    cl.loc[cl[col] > ub,'clipped_{}'.format(col)] = ub\n",
    "my_utils.plot_clipped(cl) # my_utils can be found in the github repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better way\n",
    "`.clip(lb,ub)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T20:56:36.351609Z",
     "start_time": "2018-12-11T20:56:36.004908Z"
    }
   },
   "outputs": [],
   "source": [
    "cl = pt.copy()\n",
    "# Beutiful One Liner\n",
    "cl[['clipped_A','clipped_B','clipped_C']] = cl.clip(5,12)\n",
    "my_utils.plot_clipped(cl) # my_utils can be found in the github repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reindex\n",
    "Now I have 3 event types from 17:00 to 23:00. Let's imagine, I know that actually I have 5 event types. I also know that the period was from 16:00 to 00:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:43:44.786208Z",
     "start_time": "2018-12-16T14:43:44.200628Z"
    }
   },
   "outputs": [],
   "source": [
    "etypes = list('ABCYZ') # New columns\n",
    "# Define a date range - Pandas will automatically make this into an index\n",
    "idx = pd.date_range(start='2018-10-09 16:00:00',end='2018-10-09 23:59:00',freq=pt.index.freq)\n",
    "type(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:44:54.823743Z",
     "start_time": "2018-12-16T14:44:54.791133Z"
    }
   },
   "outputs": [],
   "source": [
    "pt.reindex(index=idx, columns=etypes, fill_value=0).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:44:59.543607Z",
     "start_time": "2018-12-16T14:44:59.512909Z"
    }
   },
   "outputs": [],
   "source": [
    "### Let's put this in a function - This will help us later.\n",
    "def get_all_types_and_timestamps(df, min_date='2018-10-09 16:00:00',\n",
    "                                 max_date='2018-10-09 23:59:00', etypes=list('ABCYZ')):\n",
    "    ret = df.copy()\n",
    "    time_idx = pd.date_range(start=min_date,end=max_date,freq='15T')\n",
    "    # Indices work like set. This is a good practive so we don't override our intended index\n",
    "    idx = ret.index.union(time_idx)\n",
    "    etypes = df.columns.union(set(etypes))\n",
    "    ret = ret.reindex(idx, columns=etypes, fill_value=0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# Method Chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign\n",
    "Assign is for creating new columns on the dataframes. This is instead of\n",
    "`df[new_col] = function(df[old_col])`. They are both one lines, but `.assign` doesn't break the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:45:21.735278Z",
     "start_time": "2018-12-16T14:45:21.701466Z"
    }
   },
   "outputs": [],
   "source": [
    "pt.assign(mean_all = pt.mean(axis=1)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe\n",
    "Think R's `%>%` (Or rather, avoid thinking about R), `.pipe` is a method that accepts a function. `pipe`, by default, assumes the first argument of this function is a dataframe and passes the current dataframe down the pipeline. The function should return a dataframe also, if you want to continue with the chaining. Yet, it can also return any other value if you put it in the last step.  \n",
    "This is incredibly valueable because it takes you one step further from \"sql\" where you do things \"in reverse\".  \n",
    "$f(g(h(df)))$ = `df.pipe(h).pipe(g).pipe(f)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:46:09.390929Z",
     "start_time": "2018-12-16T14:46:09.359568Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_to_col(df, col='A', n = 200):\n",
    "    ret = df.copy()\n",
    "    # A dataframe is mutable, if you don't copy it first, this is prone to many errors.\n",
    "    # I always copy when I enter a function, even if I'm sure it shouldn't change anything.\n",
    "    ret[col] = ret[col] + n\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:46:10.001982Z",
     "start_time": "2018-12-16T14:46:09.785195Z"
    }
   },
   "outputs": [],
   "source": [
    "add_to_col(add_to_col(add_to_col(pt), 'B', 100), 'C',500).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:46:11.604691Z",
     "start_time": "2018-12-16T14:46:11.575842Z"
    }
   },
   "outputs": [],
   "source": [
    "(pt\n",
    "  .pipe(add_to_col)\n",
    "  .pipe(add_to_col, col='B', n=100)\n",
    "  .pipe(add_to_col, col='C', n=500)  \n",
    "  .head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always do this with multiple lines of `df = do_something(df)` but I think this method is more elegant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Code Tells a Story\n",
    "Your code is not just about making the computer do things. It's about telling a story of what you wish to happen. Sometimes other people will want to read you code. Most time, it is you 3 monhts in the future who will want to read it. Some say good code documents itself. I'm not that extreme, yet storytelling with code may save you from many lines of unnecessary comments.\n",
    "The next and final block tells the story in one block. It's elegant, it tells a story. If you build utility functions and `pipe` them while following meaningful naming, they help tell a story. if you `assign` columns with meaningful names, they tell a story. you `drop`, you `apply`, you `read`, you `groupby` and you `resample` - they all tell a story.\n",
    "\n",
    "(Well... Maybe they could have gone with better naming for `resample`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:49:19.659213Z",
     "start_time": "2018-12-16T14:49:18.988410Z"
    }
   },
   "outputs": [],
   "source": [
    "df = (pd\n",
    "      .read_csv    ('./data.csv', index_col=0, parse_dates=['date_'])\n",
    "      .assign      (event_type=lambda df: df.event_type.map({1: 'A', 5: 'B', 7: 'C'}))\n",
    "      .sort_values ('date_')\n",
    "      .set_index   ('date_')\n",
    "      .drop        (columns='val_updated')\n",
    "      .groupby     ('event_type')\n",
    "      .resample    ('15T')\n",
    "      .apply       ('count')\n",
    "      .rename      (columns={'event_type': 'amount'})\n",
    "      .unstack     ('event_type')\n",
    "      .pipe        (my_utils.remove_multi_index)\n",
    "      .pipe        (get_all_types_and_timestamps) # Remember this from before?\n",
    "      .assign      (mean_event=lambda x: x.mean(axis=1))\n",
    "      .loc         [:, ['mean_event']]\n",
    "      .pipe        (my_utils.make_sliding_time_windows, steps_back=6)\n",
    "      .dropna      ()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:49:19.820861Z",
     "start_time": "2018-12-16T14:49:19.787143Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus! \n",
    "Cool methods I've found but did not fit in the talk's flow.  \n",
    "\n",
    "\n",
    "<span style=\"font-size:2em\"> [No Time?](#You-don't-have-to-memorize-this)</span>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:50:08.296350Z",
     "start_time": "2018-12-16T14:50:07.949664Z"
    }
   },
   "outputs": [],
   "source": [
    "src = df.copy().loc[:,['mean_event']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T14:26:33.373610Z",
     "start_time": "2018-12-15T14:26:33.346892Z"
    }
   },
   "source": [
    "## Percent Change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:50:18.349857Z",
     "start_time": "2018-12-16T14:50:18.315779Z"
    }
   },
   "outputs": [],
   "source": [
    "src.assign(pct = src.pct_change()).head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval Index\n",
    "Helps creating a \"common language\" when talking about time series aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:55:53.910037Z",
     "start_time": "2018-12-16T14:55:53.304783Z"
    }
   },
   "outputs": [],
   "source": [
    "src = df.copy()\n",
    "ir = pd.interval_range(start=df.index.min(),\n",
    "                       end=df.index.max() + df.index.freq,\n",
    "                       freq=df.index.freq)\n",
    "type(ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:56:07.427892Z",
     "start_time": "2018-12-16T14:56:07.398189Z"
    }
   },
   "outputs": [],
   "source": [
    "src.index = ir # Interval Index\n",
    "src.loc['2018-10-09 18:37',:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:57:06.871945Z",
     "start_time": "2018-12-16T14:57:06.833077Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc['2018-10-09 18:37',:] # Datetime Index\n",
    "# Will result error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T14:39:47.808611Z",
     "start_time": "2018-12-15T14:39:47.779548Z"
    }
   },
   "source": [
    "## Split Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:57:09.830672Z",
     "start_time": "2018-12-16T14:57:09.805056Z"
    }
   },
   "outputs": [],
   "source": [
    "txt = pd.DataFrame({'text':['hello','dean langsam','diving into pandas is better than reinventing it']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:57:10.999912Z",
     "start_time": "2018-12-16T14:57:10.972212Z"
    }
   },
   "outputs": [],
   "source": [
    "txt.text.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:57:12.847418Z",
     "start_time": "2018-12-16T14:57:12.804515Z"
    }
   },
   "outputs": [],
   "source": [
    "txt.text.str.split(expand = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Examples with Pandas Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:57:35.534461Z",
     "start_time": "2018-12-16T14:57:35.465177Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas.util.testing as tm\n",
    "tm.N, tm.K = 15, 10\n",
    "st = pd.util.testing.makeTimeDataFrame() * 100\n",
    "st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research with Style!\n",
    "![so fetch](https://media.giphy.com/media/G6ojXggFcXWCs/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:57:42.513037Z",
     "start_time": "2018-12-16T14:57:42.484331Z"
    }
   },
   "outputs": [],
   "source": [
    "stnan = st.copy()\n",
    "stnan[np.random.rand(*stnan.shape) < 0.05] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:57:44.171764Z",
     "start_time": "2018-12-16T14:57:43.781280Z"
    }
   },
   "outputs": [],
   "source": [
    "(stnan\n",
    " .style\n",
    " .highlight_null('red')\n",
    " .highlight_max(color='steelblue', axis = 0)\n",
    " .highlight_min(color ='gold', axis = 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:57:51.242725Z",
     "start_time": "2018-12-16T14:57:51.097169Z"
    }
   },
   "outputs": [],
   "source": [
    "st.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:58:01.380504Z",
     "start_time": "2018-12-16T14:58:01.237925Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_style(val):\n",
    "    if val < -100:\n",
    "        return 'background-color:red'\n",
    "    elif val > 100:\n",
    "        return 'background-color:green'\n",
    "    elif abs(val) <20:\n",
    "        return 'background-color:yellow'\n",
    "    else:\n",
    "        return ''\n",
    "st.style.applymap(custom_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T14:58:24.515198Z",
     "start_time": "2018-12-16T14:58:24.432901Z"
    }
   },
   "outputs": [],
   "source": [
    "(st.style\n",
    " .bar(subset=['A','D'],color='steelblue')\n",
    " .bar(subset=['J'],color=['indianred','limegreen'], align='mid')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You don't have to memorize this\n",
    "Just put this in the back of your mind and remember that modern Pandas has so many superpowers. Just remember they exist, and google them when you actually need them.\n",
    "Always, when I feel I'm insecure about Pandas, I go back to [Greg Reda](https://twitter.com/gjreda)'s [tweet](https://twitter.com/gjreda/status/1049694953687924737):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![greg](./slides/tweet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources \n",
    "* [Modern Pandas](https://tomaugspurger.github.io/modern-1-intro.html) by Tom Augspurger\n",
    "* [Basic Time Series Manipulation with Pandas](https://towardsdatascience.com/basic-time-series-manipulation-with-pandas-4432afee64ea) by Laura Fedoruk\n",
    "* [Pandas Docs](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.clip.html). You don't have to thoroughly go over everything, just randomly open a page in the docs and you're sure to learn a new thing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "836px",
    "left": "0px",
    "right": "1233px",
    "top": "110px",
    "width": "256px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
